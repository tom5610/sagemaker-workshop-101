{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection using Amazon SageMaker built-in XGBoost - Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In many real life situations, you will encounter where you do not have labeled data and still you need to detect fraudaulent activites. This technique walks you through how to learn features, build a model using unlabeled dataset and helps you to detect frauds.\n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "The dataset has been collected and analysed during a research collaboration of Worldline and the [Machine Learning Group](http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the DefeatFraud project\n",
    "\n",
    "The dataset location is on Amazon S3: https://s3-us-west-2.amazonaws.com/sagemaker-e2e-solutions/fraud-detection/creditcardfraud.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate and process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by downloading and reading in the credit card fraud data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-22 06:06:03--  https://s3-us-west-2.amazonaws.com/sagemaker-e2e-solutions/fraud-detection/creditcardfraud.zip\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.229.176\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.229.176|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 69155632 (66M) [application/zip]\n",
      "Saving to: ‘data/creditcardfraud.zip’\n",
      "\n",
      "creditcardfraud.zip 100%[===================>]  65.95M  10.9MB/s    in 6.8s    \n",
      "\n",
      "2020-12-22 06:06:11 (9.68 MB/s) - ‘data/creditcardfraud.zip’ saved [69155632/69155632]\n",
      "\n",
      "Unzipping...\n"
     ]
    }
   ],
   "source": [
    "!wget -P data/ -N https://s3-us-west-2.amazonaws.com/sagemaker-e2e-solutions/fraud-detection/creditcardfraud.zip\n",
    "    \n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"data/creditcardfraud.zip\", 'r') as zip_ref:\n",
    "    print(\"Unzipping...\")\n",
    "    zip_ref.extractall(\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('./data/creditcard.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at our data (we only show a subset of the columns in the table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "5  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n",
       "6  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055  0.750137   \n",
       "7 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n",
       "8  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592  0.373205   \n",
       "9  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050 -0.069733   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "5  0.105915  0.253844  0.081080    3.67      0  \n",
       "6 -0.257237  0.034507  0.005168    4.99      0  \n",
       "7 -0.051634 -1.206921 -1.085339   40.80      0  \n",
       "8 -0.384157  0.011747  0.142404   93.20      0  \n",
       "9  0.094199  0.246219  0.083076    3.68      0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.columns)\n",
    "data[['Time', 'V1', 'V2', 'V27', 'V28', 'Amount', 'Class']].describe()\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class column corresponds to whether or not a transaction is fradulent. We see that the majority of data is non-fraudulant with only $492$ ($.173\\%$) of the data corresponding to fraudulant examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frauds:  492\n",
      "Number of non-frauds:  284315\n",
      "Percentage of fradulent data: 0.1727485630620034\n"
     ]
    }
   ],
   "source": [
    "nonfrauds, frauds = data.groupby('Class').size()\n",
    "print('Number of frauds: ', frauds)\n",
    "print('Number of non-frauds: ', nonfrauds)\n",
    "print('Percentage of fradulent data:', 100.*frauds/(frauds + nonfrauds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 28 columns, $V_i$ for $i=1..28$ of anonymized features along with columns for time, amount, and class. We already know that the columns $V_i$ have been normalized to have $0$ mean and unit standard deviation as the result of a PCA. You can read more about PCA here:. \n",
    "\n",
    "Tip: For our dataset this amount of preprocessing will give us reasonable accuracy, but it's important to note that there are more preprocessing steps one can use to improve accuracy . For unbalanced data sets like ours where the positive (fraudulent) examples occur much less frequently than the negative (legitimate) examples, we may try “over-sampling” the minority dataset by generating synthetic data (read about SMOTE in Data Mining for Imbalanced Datasets: An Overview (https://link.springer.com/chapter/10.1007%2F0-387-25465-X_40) or undersampling the majority class by using ensemble methods (see http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.6858&rep=rep1&type=pdfor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = data.columns[:-1]\n",
    "label_column = data.columns[-1]\n",
    "\n",
    "features = data[feature_columns].values.astype('float32')\n",
    "labels = (data[label_column].values).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some analysis and discuss different ways we can preprocess our data. Let's discuss the way in which this data was preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data and Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Amazon common libraries provide utilities to convert NumPy n-dimensional arrays into a the Record-IO format which SageMaker uses for a concise representation of features and labels. The Record-IO format is implemented via protocol buffer so the serialization is very efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0      0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1      0   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2      0   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3      0   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4      0   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8  ...       V20       V21       V22       V23       V24  \\\n",
       "0  0.239599  0.098698  ...  0.251412 -0.018307  0.277838 -0.110474  0.066928   \n",
       "1 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.791461  0.247676  ...  0.524980  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.237609  0.377436  ... -0.208038 -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = data\n",
    "model_data.head()\n",
    "model_data = pd.concat([model_data['Class'], model_data.drop(['Class'], axis=1)], axis=1)\n",
    "model_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we upload the data to S3 using boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "sagemaker_iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "prefix = 'sagemaker/DEMO-xgboost-fraud'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data location: s3://sagemaker-ap-southeast-2-593380422482/sagemaker/DEMO-xgboost-fraud/train/train.csv\n",
      "Uploaded training data location: s3://sagemaker-ap-southeast-2-593380422482/sagemaker/DEMO-xgboost-fraud/validation/validation.csv\n",
      "Training artifacts will be uploaded to: s3://sagemaker-ap-southeast-2-593380422482/sagemaker/DEMO-xgboost-fraud/output\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), \n",
    "                                                  [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)\n",
    "\n",
    "\n",
    "boto_session.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')) \\\n",
    "                                .upload_file('train.csv')\n",
    "boto_session.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')) \\\n",
    "                                .upload_file('validation.csv')\n",
    "s3_train_data = 's3://{}/{}/train/train.csv'.format(bucket, prefix)\n",
    "s3_validation_data = 's3://{}/{}/validation/validation.csv'.format(bucket, prefix)\n",
    "print('Uploaded training data location: {}'.format(s3_train_data))\n",
    "print('Uploaded training data location: {}'.format(s3_validation_data))\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('Training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Moving onto training, first we'll need to specify the locations of the XGBoost algorithm containers.\n",
    "To specify the Linear Learner algorithm, we use a utility function to obtain it's URI. A complete list of build-in algorithms is found here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region=region, version=\"1.2-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create s3_inputs that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "\n",
    "More detail on XGBoost's hyperparmeters can be found on their GitHub [page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker abstracts training with Estimators. We can pass container, and all parameters to the estimator, as well as the hyperparameters for the linear learner and fit the estimator to the data in S3.\n",
    "Note: For IP protection reasons, SageMaker built-in algorithms, such as XGBoost, can't be run locally, i.e. on the same instance where this Jupyter Notebook code is running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role = sagemaker_iam_role, \n",
    "                                    instance_count = 1, \n",
    "                                    instance_type = 'ml.m5.xlarge',\n",
    "                                    output_path = output_location,\n",
    "                                    sagemaker_session = session,\n",
    "                                    max_run = 20 * 60,\n",
    "                                    use_spot_instances = True,\n",
    "                                    max_wait = 20 * 60) # wait for 300s on managed spot training, if fails, use on-demand\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-22 06:46:09 Starting - Starting the training job...\n",
      "2020-12-22 06:46:10 Starting - Launching requested ML instancesProfilerReport-1608619568: InProgress\n",
      "......\n",
      "2020-12-22 06:47:22 Starting - Preparing the instances for training......\n",
      "2020-12-22 06:48:34 Downloading - Downloading input data...\n",
      "2020-12-22 06:49:04 Training - Training image download completed. Training in progress...\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 199364 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 56962 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.00064#011validation-error:0.00063\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.00052#011validation-error:0.00058\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.00056#011validation-error:0.00056\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.00051#011validation-error:0.00058\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.00050#011validation-error:0.00054\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.00048#011validation-error:0.00054\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.00047#011validation-error:0.00054\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.00049#011validation-error:0.00054\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.00047#011validation-error:0.00056\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.00045#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.00046#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.00045#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.00045#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.00043#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.00042#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.00042#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.00041#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.00041#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.00041#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.00041#011validation-error:0.00046\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.00040#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.00040#011validation-error:0.00053\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.00040#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.00040#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.00040#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.00039#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.00039#011validation-error:0.00053\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.00038#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.00037#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.00037#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.00037#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.00036#011validation-error:0.00046\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.00035#011validation-error:0.00046\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.00036#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.00036#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.00036#011validation-error:0.00046\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.00035#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.00035#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.00034#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.00034#011validation-error:0.00056\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.00033#011validation-error:0.00053\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.00034#011validation-error:0.00053\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.00032#011validation-error:0.00053\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.00032#011validation-error:0.00053\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.00033#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.00032#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.00032#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.00032#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.00033#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.00032#011validation-error:0.00053\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.00032#011validation-error:0.00053\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.00032#011validation-error:0.00053\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.00032#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.00031#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.00031#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.00032#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.00031#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.00030#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.00031#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.00031#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.00031#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.00031#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.00031#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.00030#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.00030#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.00030#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.00031#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.00031#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.00030#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.00030#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.00030#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.00030#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.00030#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.00030#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.00029#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.00029#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.00029#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.00029#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.00029#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.00029#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.00029#011validation-error:0.00047\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.00029#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.00029#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.00029#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.00030#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.00029#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.00029#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.00029#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.00029#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.00029#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.00028#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.00027#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.00027#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.00027#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.00026#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.00026#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.00026#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.00026#011validation-error:0.00049\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.00025#011validation-error:0.00051\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.00025#011validation-error:0.00051\u001b[0m\n",
      "\n",
      "2020-12-22 06:49:32 Uploading - Uploading generated training model\n",
      "2020-12-22 06:49:32 Completed - Training job completed\n",
      "Training seconds: 58\n",
      "Billable seconds: 18\n",
      "Managed Spot Training savings: 69.0%\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deploy the estimator to and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "xgb.name = 'deployed-xgboost-fraud-prediction'\n",
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m5.xlarge',\n",
    "                          endpoint_name='deployed-xgboost-fraud-prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, \n",
    "simply by making an http POST request.  But first, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "xgb_predictor.deserializer = sagemaker.deserializers.CSVDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batchs to CSV string payloads\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156699</th>\n",
       "      <td>0</td>\n",
       "      <td>108720.0</td>\n",
       "      <td>1.766913</td>\n",
       "      <td>0.251711</td>\n",
       "      <td>-0.501575</td>\n",
       "      <td>4.214333</td>\n",
       "      <td>0.152405</td>\n",
       "      <td>-0.054836</td>\n",
       "      <td>0.066733</td>\n",
       "      <td>-0.142544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269055</td>\n",
       "      <td>-0.045084</td>\n",
       "      <td>0.070225</td>\n",
       "      <td>0.031831</td>\n",
       "      <td>-0.117100</td>\n",
       "      <td>0.049678</td>\n",
       "      <td>0.056044</td>\n",
       "      <td>-0.075564</td>\n",
       "      <td>-0.046625</td>\n",
       "      <td>84.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189453</th>\n",
       "      <td>0</td>\n",
       "      <td>128407.0</td>\n",
       "      <td>0.244865</td>\n",
       "      <td>-2.935385</td>\n",
       "      <td>-2.641925</td>\n",
       "      <td>1.212391</td>\n",
       "      <td>-0.927223</td>\n",
       "      <td>-0.887115</td>\n",
       "      <td>1.258957</td>\n",
       "      <td>-0.441144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.611841</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>-0.996756</td>\n",
       "      <td>-0.611389</td>\n",
       "      <td>-0.179716</td>\n",
       "      <td>-0.440728</td>\n",
       "      <td>-0.175587</td>\n",
       "      <td>-0.183370</td>\n",
       "      <td>0.135004</td>\n",
       "      <td>926.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171632</th>\n",
       "      <td>0</td>\n",
       "      <td>120743.0</td>\n",
       "      <td>-1.845167</td>\n",
       "      <td>1.067365</td>\n",
       "      <td>-0.305292</td>\n",
       "      <td>0.477415</td>\n",
       "      <td>2.378210</td>\n",
       "      <td>-0.308770</td>\n",
       "      <td>1.345665</td>\n",
       "      <td>-0.432156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294273</td>\n",
       "      <td>-0.180982</td>\n",
       "      <td>-0.028561</td>\n",
       "      <td>-0.967699</td>\n",
       "      <td>0.135233</td>\n",
       "      <td>1.180916</td>\n",
       "      <td>-0.367559</td>\n",
       "      <td>-0.451136</td>\n",
       "      <td>0.062387</td>\n",
       "      <td>44.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271641</th>\n",
       "      <td>0</td>\n",
       "      <td>164670.0</td>\n",
       "      <td>2.008275</td>\n",
       "      <td>-0.562768</td>\n",
       "      <td>-0.591703</td>\n",
       "      <td>0.380489</td>\n",
       "      <td>-0.591979</td>\n",
       "      <td>-0.215331</td>\n",
       "      <td>-0.699080</td>\n",
       "      <td>0.054018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262080</td>\n",
       "      <td>0.190666</td>\n",
       "      <td>0.736771</td>\n",
       "      <td>-0.003363</td>\n",
       "      <td>-0.716620</td>\n",
       "      <td>0.004677</td>\n",
       "      <td>-0.106274</td>\n",
       "      <td>0.030152</td>\n",
       "      <td>-0.046083</td>\n",
       "      <td>19.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97357</th>\n",
       "      <td>0</td>\n",
       "      <td>66183.0</td>\n",
       "      <td>0.678375</td>\n",
       "      <td>-1.426937</td>\n",
       "      <td>0.669737</td>\n",
       "      <td>0.229416</td>\n",
       "      <td>-1.267753</td>\n",
       "      <td>0.621544</td>\n",
       "      <td>-0.697380</td>\n",
       "      <td>0.447915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236969</td>\n",
       "      <td>-0.059631</td>\n",
       "      <td>-0.590134</td>\n",
       "      <td>-0.124976</td>\n",
       "      <td>-0.281126</td>\n",
       "      <td>-0.100933</td>\n",
       "      <td>0.957737</td>\n",
       "      <td>-0.084522</td>\n",
       "      <td>0.034037</td>\n",
       "      <td>250.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109950</th>\n",
       "      <td>0</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>1.324629</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>0.112098</td>\n",
       "      <td>-0.443294</td>\n",
       "      <td>-0.259921</td>\n",
       "      <td>-0.058018</td>\n",
       "      <td>-0.413213</td>\n",
       "      <td>0.119846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090432</td>\n",
       "      <td>-0.172716</td>\n",
       "      <td>-0.565828</td>\n",
       "      <td>-0.044942</td>\n",
       "      <td>-0.805622</td>\n",
       "      <td>0.206340</td>\n",
       "      <td>0.963207</td>\n",
       "      <td>-0.083072</td>\n",
       "      <td>-0.015200</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257557</th>\n",
       "      <td>0</td>\n",
       "      <td>158224.0</td>\n",
       "      <td>-0.761115</td>\n",
       "      <td>0.752660</td>\n",
       "      <td>-0.588619</td>\n",
       "      <td>-0.735394</td>\n",
       "      <td>1.047455</td>\n",
       "      <td>-0.687428</td>\n",
       "      <td>1.506912</td>\n",
       "      <td>-0.430123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.467518</td>\n",
       "      <td>0.174627</td>\n",
       "      <td>0.730944</td>\n",
       "      <td>-0.031881</td>\n",
       "      <td>-0.358848</td>\n",
       "      <td>-0.429493</td>\n",
       "      <td>-0.423124</td>\n",
       "      <td>-0.276905</td>\n",
       "      <td>0.242209</td>\n",
       "      <td>49.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185893</th>\n",
       "      <td>0</td>\n",
       "      <td>126867.0</td>\n",
       "      <td>0.046435</td>\n",
       "      <td>0.778347</td>\n",
       "      <td>0.271485</td>\n",
       "      <td>-0.571705</td>\n",
       "      <td>0.285194</td>\n",
       "      <td>-1.161162</td>\n",
       "      <td>0.923525</td>\n",
       "      <td>-0.134226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.125217</td>\n",
       "      <td>-0.275709</td>\n",
       "      <td>-0.676042</td>\n",
       "      <td>0.083668</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>-0.534995</td>\n",
       "      <td>0.142126</td>\n",
       "      <td>0.239108</td>\n",
       "      <td>0.094581</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68710</th>\n",
       "      <td>0</td>\n",
       "      <td>53109.0</td>\n",
       "      <td>0.601423</td>\n",
       "      <td>0.531141</td>\n",
       "      <td>0.589295</td>\n",
       "      <td>0.888435</td>\n",
       "      <td>-0.432970</td>\n",
       "      <td>-0.320487</td>\n",
       "      <td>0.119739</td>\n",
       "      <td>-0.076921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130512</td>\n",
       "      <td>-0.092272</td>\n",
       "      <td>-0.052984</td>\n",
       "      <td>0.437723</td>\n",
       "      <td>0.431241</td>\n",
       "      <td>-1.139842</td>\n",
       "      <td>0.036512</td>\n",
       "      <td>-0.282478</td>\n",
       "      <td>-0.132820</td>\n",
       "      <td>12.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>-0.816884</td>\n",
       "      <td>0.984619</td>\n",
       "      <td>1.316099</td>\n",
       "      <td>-0.726893</td>\n",
       "      <td>0.105275</td>\n",
       "      <td>-0.490752</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.046676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133314</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.499736</td>\n",
       "      <td>-0.216309</td>\n",
       "      <td>-0.124242</td>\n",
       "      <td>0.122218</td>\n",
       "      <td>0.416430</td>\n",
       "      <td>0.386121</td>\n",
       "      <td>0.219177</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28481 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class      Time        V1        V2        V3        V4        V5  \\\n",
       "156699      0  108720.0  1.766913  0.251711 -0.501575  4.214333  0.152405   \n",
       "189453      0  128407.0  0.244865 -2.935385 -2.641925  1.212391 -0.927223   \n",
       "171632      0  120743.0 -1.845167  1.067365 -0.305292  0.477415  2.378210   \n",
       "271641      0  164670.0  2.008275 -0.562768 -0.591703  0.380489 -0.591979   \n",
       "97357       0   66183.0  0.678375 -1.426937  0.669737  0.229416 -1.267753   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "109950      0   71613.0  1.324629 -0.181641  0.112098 -0.443294 -0.259921   \n",
       "257557      0  158224.0 -0.761115  0.752660 -0.588619 -0.735394  1.047455   \n",
       "185893      0  126867.0  0.046435  0.778347  0.271485 -0.571705  0.285194   \n",
       "68710       0   53109.0  0.601423  0.531141  0.589295  0.888435 -0.432970   \n",
       "1677        0    1301.0 -0.816884  0.984619  1.316099 -0.726893  0.105275   \n",
       "\n",
       "              V6        V7        V8  ...       V20       V21       V22  \\\n",
       "156699 -0.054836  0.066733 -0.142544  ... -0.269055 -0.045084  0.070225   \n",
       "189453 -0.887115  1.258957 -0.441144  ...  1.611841  0.298892 -0.996756   \n",
       "171632 -0.308770  1.345665 -0.432156  ... -0.294273 -0.180982 -0.028561   \n",
       "271641 -0.215331 -0.699080  0.054018  ... -0.262080  0.190666  0.736771   \n",
       "97357   0.621544 -0.697380  0.447915  ...  0.236969 -0.059631 -0.590134   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "109950 -0.058018 -0.413213  0.119846  ... -0.090432 -0.172716 -0.565828   \n",
       "257557 -0.687428  1.506912 -0.430123  ... -0.467518  0.174627  0.730944   \n",
       "185893 -1.161162  0.923525 -0.134226  ... -0.125217 -0.275709 -0.676042   \n",
       "68710  -0.320487  0.119739 -0.076921  ... -0.130512 -0.092272 -0.052984   \n",
       "1677   -0.490752  0.697917  0.046676  ...  0.133314 -0.272599 -0.499736   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \n",
       "156699  0.031831 -0.117100  0.049678  0.056044 -0.075564 -0.046625   84.22  \n",
       "189453 -0.611389 -0.179716 -0.440728 -0.175587 -0.183370  0.135004  926.67  \n",
       "171632 -0.967699  0.135233  1.180916 -0.367559 -0.451136  0.062387   44.12  \n",
       "271641 -0.003363 -0.716620  0.004677 -0.106274  0.030152 -0.046083   19.95  \n",
       "97357  -0.124976 -0.281126 -0.100933  0.957737 -0.084522  0.034037  250.88  \n",
       "...          ...       ...       ...       ...       ...       ...     ...  \n",
       "109950 -0.044942 -0.805622  0.206340  0.963207 -0.083072 -0.015200    0.75  \n",
       "257557 -0.031881 -0.358848 -0.429493 -0.423124 -0.276905  0.242209   49.99  \n",
       "185893  0.083668  0.003889 -0.534995  0.142126  0.239108  0.094581    2.28  \n",
       "68710   0.437723  0.431241 -1.139842  0.036512 -0.282478 -0.132820   12.95  \n",
       "1677   -0.216309 -0.124242  0.122218  0.416430  0.386121  0.219177    5.00  \n",
       "\n",
       "[28481 rows x 31 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(xgb_predictor, data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        result = xgb_predictor.predict(array)\n",
    "        predictions = ','.join([predictions, ','.join(result[0])])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(xgb_predictor, test_data.values[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values. In this case, we're simply predicting whether the customer churned (1) or not (0), which produces a simple confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frauds:  54\n",
      "Number of non-frauds:  28427\n",
      "Percentage of fradulent data: 0.1896000842667041\n"
     ]
    }
   ],
   "source": [
    "test_nonfrauds, test_frauds = test_data.groupby('Class').size()\n",
    "print('Number of frauds: ', test_frauds)\n",
    "print('Number of non-frauds: ', test_nonfrauds)\n",
    "print('Percentage of fradulent data:', 100.*test_frauds/(test_frauds + test_nonfrauds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions    0.0  1.0\n",
       "actual                 \n",
       "0            28426    1\n",
       "1               12   42"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision: tp / (tp + fp)\n",
    "#recall: tp / (tp + fn)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "results = precision_recall_fscore_support(test_data.iloc[:, 0],\n",
    "                                         np.round(predictions))\n",
    "print('precision: ', round(results[0][1], 2))\n",
    "print('recall: ', round(results[1][1], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, due to randomized elements of the algorithm, you results may differ slightly.\n",
    "\n",
    "Of the 54 fraudsters, we've correctly predicted 40 of them (true positives). And, we incorrectly predicted 1 case of fraud (false positive). There are also 14 cases of fraud that the model classified as benign transaction (false negatives) - which can get really expensive.\n",
    "\n",
    "An important point here is that because of the np.round() function above we are using a simple threshold (or cutoff) of 0.5. Our predictions from xgboost come out as continuous values between 0 and 1 and we force them into the binary classes that we began with. So, we should consider adjusting this cutoff. That will almost certainly increase the number of false positives, but it can also be expected to increase the number of true positives and reduce the number of false negatives.\n",
    "\n",
    "To get a rough intuition here, let's look at the continuous values of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the cutoff threshold, we can trade false positives for false negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28412</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      0   1\n",
       "Class           \n",
       "0      28412  15\n",
       "1          8  46"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.04, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.75\n",
      "recall:  0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "results = precision_recall_fscore_support(test_data.iloc[:, 0],\n",
    "                                         np.where(predictions > 0.04, 1, 0))\n",
    "print('precision: ', round(results[0][1], 2))\n",
    "print('recall: ', round(results[1][1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative cost of errors\n",
    "\n",
    "Any practical binary classification problem is likely to produce a similarly sensitive cutoff. \n",
    "If we put an ML model into production, there are costs associated with the model erroneously assigning false positives and false negatives. Because the choice of the cutoff affects all four of these statistics, we need to consider the relative costs to the business for each of these four outcomes for each prediction.\n",
    "\n",
    "#### Assigning costs\n",
    "\n",
    "What are the costs for our problem fraud detection? The costs, of course, depend on the specific actions that the business takes. Let's make some assumptions here.\n",
    "\n",
    "First, assign the cost of \\$0.00 to both the true negatives (correctly recognized benign transactions) and true positives (correctly recognized fraudulent transactions). Our model essentially correctly identified both situations. One can assign a benefit (i.e. negative cost) to correctly detected fraud, but we are not going to do this here.\n",
    "\n",
    "False negatives are the most problematic, because they represent a fraudulent transactions that slipped through our model. Based on some Internet research (see sources below), we assign a cost of \\$450.00 for each one. This is the cost of false negatives.\n",
    "\n",
    "Finally, False positives are the genuine transactions that our model would block as fraud. This would result in an annoyed customer that might possibly close the credit card account and move to another bank. We assume that it costs a \\$500.00 sign-on bonus to obtain a cr. card customer and that \\5 percent of annoyed customers would defect. \n",
    "\n",
    "Source:\n",
    "\n",
    "https://www.creditcards.com/credit-card-news/credit-card-security-id-theft-fraud-statistics-1276.php\n",
    "https://wallethub.com/edu/cc/credit-debit-card-fraud-statistics/25725/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal cutoff\n",
    "\n",
    "It’s clear that false negatives are substantially more costly than false positives. We should be minimizing a cost function that looks like this:\n",
    "\n",
    "```txt\n",
    "$450 * FN(C) + $0 * TN(C) + 0.05*$500 * FP(C) + $0 * TP(C)\n",
    "```\n",
    "\n",
    "FN(C) means that the false negative percentage is a function of the cutoff, C, and similar for TN, FP, and TP.  We need to find the cutoff, C, where the result of the expression is smallest.\n",
    "\n",
    "A straightforward way to do this, is to simply run a simulation over a large number of possible cutoffs.  We test 100 possible values in the for loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is minimized near a cutoff of: 0.2\n"
     ]
    }
   ],
   "source": [
    "TN_cost = 0\n",
    "TP_cost = 0\n",
    "FP_cost = 0.05*500 #$cost of losing an annoyed customer (assuming 5% defection and $500 sign-on bonus)\n",
    "FN_cost = 450 # $cost of of letting a fradulent transaction slip through\n",
    "\n",
    "cutoffs = np.arange(0.01, 1, 0.01)\n",
    "costs = []\n",
    "for c in cutoffs:\n",
    "    costs.append(np.sum(np.sum(np.array([[TN_cost, FP_cost], [FN_cost, TP_cost]]) * \n",
    "                               pd.crosstab(index=test_data.iloc[:, 0], \n",
    "                                           columns=np.where(predictions > c, 1, 0)))))\n",
    "\n",
    "costs = np.array(costs)\n",
    "plt.plot(cutoffs, costs)\n",
    "plt.show()\n",
    "print('Cost is minimized near a cutoff of:', cutoffs[np.argmin(costs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [Optional] Hyperparameter Optimization (HPO)\n",
    "*Note, with the default settings below, the hyperparameter tuning job can take up to 20~30 minutes to complete.*\n",
    "\n",
    "We will use SageMaker HyperParameter Optimization (HPO) to automate the searching process effectively. Specifically, we **specify a range**, or a list of possible values in the case of categorical hyperparameters, for each of the hyperparameter that we plan to tune.\n",
    "\n",
    "SageMaker hyperparameter tuning will automatically launch **multiple training jobs** with different hyperparameter settings, evaluate results of those training jobs based on a predefined \"objective metric\", and select the hyperparameter settings for future attempts based on previous results. For each hyperparameter tuning job, we will specify the maximum number of HPO tries (`max_jobs`) and how many of these can happen in parallel (`max_parallel_jobs`).\n",
    "\n",
    "Tip: `max_parallel_jobs` creates a **trade-off between performance and speed** (better hyperparameter values vs how long it takes to find these values). If `max_parallel_jobs` is large, then HPO is faster, but the discovered values may not be optimal. Smaller `max_parallel_jobs` will increase the chance of finding optimal values, but HPO will take more time to finish.\n",
    "\n",
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. Since we are using built-in XGBoost algorithm here, it emits two predefined metrics: **validation:auc** and **train:auc**, and we elected to monitor *validation:auc* as you can see below. In this case (because it's pre-built for us), we only need to specify the metric name.\n",
    "\n",
    "For more information on the documentation of the Sagemaker HPO please refer [here](https://sagemaker.readthedocs.io/en/stable/tuner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required HPO objects\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# set up hyperparameter ranges\n",
    "ranges = {\n",
    "    \"num_round\": IntegerParameter(1, 300),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 5),\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "}\n",
    "\n",
    "# set up the objective metric\n",
    "objective = \"validation:auc\"\n",
    "\n",
    "# instantiate a HPO object\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator = xgb,              # the SageMaker estimator object\n",
    "    hyperparameter_ranges=ranges,     # the range of hyperparameters\n",
    "    max_jobs = 12,                      # total number of HPO jobs\n",
    "    max_parallel_jobs = 3,              # how many HPO jobs can run in parallel\n",
    "    strategy=\"Bayesian\",              # the internal optimization strategy of HPO\n",
    "    objective_metric_name = objective,  # the objective metric to be used for HPO\n",
    "    objective_type = \"Maximize\",        # maximize or minimize the objective metric\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch HPO\n",
    "Now we can launch a hyperparameter tuning job by calling *fit()* function. After the hyperparameter tuning job is created, we can go to SageMaker console to track the progress of the hyperparameter tuning job until it is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................."
     ]
    }
   ],
   "source": [
    "# start HPO\n",
    "tuner.fit({ \"train\": s3_input_train, \"validation\": s3_input_validation })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPO jobs often take quite a long time to finish and as such, sometimes you may want to free up the notebook and then resume the wait later.\n",
    "\n",
    "Just like the Estimator, we won't be able to `deploy()` the model until the HPO tuning job is complete; and the status is visible through both the [AWS Console](https://console.aws.amazon.com/sagemaker/home?#/hyper-tuning-jobs) and the [SageMaker API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeHyperParameterTuningJob.html). We could for example write a polling script like the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait, until HPO is finished\n",
    "hpo_state = None\n",
    "\n",
    "while hpo_state is None or hpo_state == \"InProgress\":\n",
    "    if hpo_state is not None:\n",
    "        print(\"-\", end=\"\")\n",
    "        time.sleep(60)  # Poll once every 1 min\n",
    "    hpo_state = sgmk_client.describe_hyper_parameter_tuning_job(\n",
    "        HyperParameterTuningJobName=tuner.latest_tuning_job.job_name,\n",
    "    )[\"HyperParameterTuningJobStatus\"]\n",
    "\n",
    "print(\"\\nHPO state:\", hpo_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deploy and test optimized model\n",
    "Deploying the best model is another simple `.deploy()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the best model from HPO\n",
    "hpo_predictor = tuner.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    serializer=sagemaker.serializers.CSVSerializer(),\n",
    "    deserializer=sagemaker.deserializers.CSVDeserializer(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(hpo_predictor, test_data.values[:, 1:])\n",
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "We will leave the prediction endpoint running at the end of this notebook so we can handle incoming event streams. However, don't forget to delete the prediction endpoint when you're done. You can do that at the Amazon SageMaker console in the Endpoints page. Or you can run `xgb_predictor.delete_endpoint()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Acknowledgements\n",
    "\n",
    "The dataset used to demonstrated the fraud detection solution has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the [DefeatFraud](https://mlg.ulb.ac.be/wordpress/portfolio_page/defeatfraud-assessment-and-validation-of-deep-feature-engineering-and-learning-solutions-for-fraud-detection/) project\n",
    "We cite the following works:\n",
    "* Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "* Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
    "* Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
    "* Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
    "* Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Aël; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
    "* Carcillo, Fabrizio; Le Borgne, Yann-Aël; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
