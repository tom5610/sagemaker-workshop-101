{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Athena SQL Model\n",
    "\n",
    "This example will create an athena table for [Jan 2017 taxi dataset](https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/).  You can improve performance if you convert into a parquet format.\n",
    "\n",
    "Configure your notebook role with permissions to [query data from athena](https://aws.amazon.com/blogs/machine-learning/run-sql-queries-from-your-sagemaker-notebooks-using-amazon-athena/) and access the s3 staging bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries\n",
    "\n",
    "Install the [Athena library](https://pypi.org/project/PyAthena/) for python and [tqdm](https://tqdm.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U pip\n",
    "!{sys.executable} -m pip install -U pandas\n",
    "!{sys.executable} -m pip install -U PyAthena[Pandas]==1.11.2\n",
    "!{sys.executable} -m pip install -U tqdm\n",
    "!{sys.executable} -m pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Kernel\n",
    "\n",
    "Now that you have upgraded SageMaker you need to restart the kernel by clicking menu: `Kernel -> Restart & Clear Output`.\n",
    "\n",
    "Once restarted, run the next cell to check you have version starting with `2.x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip show sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "Create an anthena database and external table for the imported nyc bit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Initialize the boto session in us-east-1 region\n",
    "boto_session = boto3.session.Session(region_name='us-east-1')\n",
    "region = boto_session.region_name\n",
    "bucket = sagemaker.session.Session(boto_session).default_bucket()\n",
    "\n",
    "# Get the athena staging dir andtable\n",
    "s3_staging_dir = 's3://{}/athena'.format(bucket)\n",
    "db_name = 'nyc_taxi'\n",
    "table_name = '{}.taxi_csv'.format(db_name)\n",
    "\n",
    "print('s3 staging dir: {}'.format(s3_staging_dir))\n",
    "print('athena table: {}'.format(table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the bucket if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 mb s3://$bucket --region $region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the nyc taxi dataset using [PandasCursor](https://pypi.org/project/PyAthena/#pandascursor) for improved performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "from pyathena.pandas_cursor import PandasCursor\n",
    "import pandas as pd\n",
    "\n",
    "cursor = connect(s3_staging_dir=s3_staging_dir,\n",
    "                 region_name=region,\n",
    "                 cursor_class=PandasCursor).cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_ddl_create_table = 'CREATE DATABASE IF NOT EXISTS {};'.format(db_name)\n",
    "\n",
    "cursor.execute(sql_ddl_create_table)\n",
    "print('Status: {}, Run time: {:.2f}s'.format(cursor.state, \n",
    "    cursor.execution_time_in_millis/1000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_create_table = '''\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS `{}` (\n",
    "    `vendorid` bigint, \n",
    "    `lpep_pickup_datetime` string, \n",
    "    `lpep_dropoff_datetime` string, \n",
    "    `store_and_fwd_flag` string, \n",
    "    `ratecodeid` bigint, \n",
    "    `pulocationid` bigint, \n",
    "    `dolocationid` bigint, \n",
    "    `passenger_count` bigint, \n",
    "    `trip_distance` double, \n",
    "    `fare_amount` double, \n",
    "    `extra` double, \n",
    "    `mta_tax` double, \n",
    "    `tip_amount` double, \n",
    "    `tolls_amount` double, \n",
    "    `ehail_fee` string, \n",
    "    `improvement_surcharge` double, \n",
    "    `total_amount` double, \n",
    "    `payment_type` bigint, \n",
    "    `trip_type` bigint)\n",
    "ROW FORMAT DELIMITED \n",
    "    FIELDS TERMINATED BY ',' \n",
    "STORED AS INPUTFORMAT \n",
    "    'org.apache.hadoop.mapred.TextInputFormat' \n",
    "OUTPUTFORMAT \n",
    "    'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION\n",
    "    's3://aws-bigdata-blog/artifacts/glue-data-lake/data/'\n",
    "TBLPROPERTIES (\n",
    "    'columnsOrdered'='true', \n",
    "    'compressionType'='none', \n",
    "    'skip.header.line.count'='1')\n",
    "'''.format(table_name)\n",
    "\n",
    "cursor.execute(sql_create_table)\n",
    "print('Status: {}, Run time: {:.2f}s'.format(cursor.state, \n",
    "    cursor.execution_time_in_millis/1000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sql = '''\n",
    "SELECT \n",
    "    total_amount, fare_amount, lpep_pickup_datetime, lpep_dropoff_datetime, trip_distance \n",
    "FROM {} WHERE total_amount is not null;\n",
    "'''.format(table_name)\n",
    "print('Querying...', data_sql)\n",
    "\n",
    "data_df = cursor.execute(data_sql).as_pandas()\n",
    "print('Status: {}, Run time: {:.2f}s, Data scanned: {:.2f}MB, Records: {:,}'.format(cursor.state, \n",
    "    cursor.execution_time_in_millis/1000.0, cursor.data_scanned_in_bytes/1024.0/1024.0, data_df.shape[0]))\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance some simple feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some date features\n",
    "data_df['lpep_pickup_datetime'] = data_df['lpep_pickup_datetime'].astype('datetime64[ns]')\n",
    "data_df['lpep_dropoff_datetime'] = data_df['lpep_dropoff_datetime'].astype('datetime64[ns]')\n",
    "data_df['duration_minutes'] = (data_df['lpep_dropoff_datetime'] - data_df['lpep_pickup_datetime']).dt.seconds/60\n",
    "data_df['hour_of_day'] = data_df['lpep_pickup_datetime'].dt.hour\n",
    "data_df['day_of_week'] = data_df['lpep_pickup_datetime'].dt.dayofweek\n",
    "data_df['week_of_year'] = data_df['lpep_pickup_datetime'].dt.weekofyear\n",
    "data_df['month_of_year'] = data_df['lpep_pickup_datetime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude any outliers\n",
    "data_df = data_df[(data_df.total_amount > 0) & (data_df.total_amount < 200) & \n",
    "                  (data_df.duration_minutes > 0) & (data_df.duration_minutes < 120) & \n",
    "                  (data_df.trip_distance > 0) & (data_df.trip_distance < 1000)].dropna()\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Build an XGBoost model to predict the total amount based on some fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(boto_session)\n",
    "role = sagemaker.get_execution_role()\n",
    "prefix = 'nyc-taxi'\n",
    "\n",
    "print('bucket: {}, prefix: {}'.format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_cols = ['total_amount', 'duration_minutes', 'trip_distance', 'hour_of_day']\n",
    "train_df, val_df = train_test_split(data_df[train_cols], test_size=0.20, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.50, random_state=42)\n",
    "\n",
    "print('split train: {}, val: {}, test: {} '.format(train_df.shape[0], val_df.shape[0], test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index and save files with target as first column\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data\n",
    "\n",
    "Save train and validation as CSV with `total_amount` as first col but no headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the tpep_pickup_datetime and save\n",
    "train_df.to_csv('train.csv', index=False, header=False)\n",
    "val_df.to_csv('validation.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Uplaod the files to s3 \n",
    "s3_train_uri = sagemaker_session.upload_data('train.csv', bucket, prefix + '/data/training')\n",
    "s3_val_uri = sagemaker_session.upload_data('validation.csv', bucket, prefix + '/data/validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate that we have uploaded these files succesfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $s3_train_uri \n",
    "!aws s3 ls $s3_val_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(region=region, framework=\"xgboost\", version=\"latest\")\n",
    "print('container: {}'.format(container))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('output: {}'.format(output_path))\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role,\n",
    "                                    instance_count=1,\n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(max_depth=9,\n",
    "                        eta=0.2, \n",
    "                        gamma=4,\n",
    "                        min_child_weight=300,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='reg:linear',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=10000)\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=s3_train_uri, content_type=\"csv\")\n",
    "s3_input_val = sagemaker.inputs.TrainingInput(s3_data=s3_val_uri, content_type=\"csv\")\n",
    "\n",
    "xgb.fit({'train': s3_input_train,  'validation': s3_input_val})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name='xgb-athena-integration-endpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalulate Model\n",
    "\n",
    "Get predicitons for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "xgb_predictor.serializer = CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in tqdm(split_array):\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "# Get predictions and store in df\n",
    "predictions = predict(val_df[train_cols[1:]].values)\n",
    "predictions = pd.DataFrame({'total_amount_predictions': predictions })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the abs error between predictions\n",
    "pred_df = val_df.join(predictions)\n",
    "pred_df['error'] = abs(pred_df['total_amount']-pred_df['total_amount_predictions'])\n",
    "pred_df.sort_values('error', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Athena UDF \n",
    "\n",
    "Create a [User Defined Function](https://aws.amazon.com/blogs/big-data/prepare-data-for-model-training-and-invoke-machine-learning-models-with-amazon-athena/) for the deployed endpoint so you can query directly in Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = xgb_predictor.endpoint_name\n",
    "print('endpoint: {}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NOTE`: Athena ML is [in preview](https://aws.amazon.com/athena/faqs/#Preview_features).   To enable this Preview feature you need to create an Athena workgroup named `AmazonAthenaPreviewFunctionality` and run any queries attempting to federate to this connector, use a UDF, or SageMaker inference from that workgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workgroup_name = 'AmazonAthenaPreviewFunctionality'\n",
    "\n",
    "!aws athena create-work-group --name $workgroup_name --region $region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using presto [datetime](https://prestodb.io/docs/0.172/functions/datetime.html) functions with inline query, rank by absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sql  = '''\n",
    "USING FUNCTION predict_total(\n",
    "  duration_minutes DOUBLE, \n",
    "  trip_distance DOUBLE, \n",
    "  hour_of_day DOUBLE) returns DOUBLE type SAGEMAKER_INVOKE_ENDPOINT\n",
    "WITH (sagemaker_endpoint='{}')\n",
    "\n",
    "SELECT \n",
    "    *, ABS(predicted_total_amount-total_amount) as error\n",
    "FROM ( \n",
    "    SELECT\n",
    "        *,\n",
    "        predict_total(duration_minutes, trip_distance, hour_of_day) as predicted_total_amount\n",
    "    FROM \n",
    "    (\n",
    "        SELECT \n",
    "            total_amount,\n",
    "            CAST(date_diff('minute', \n",
    "                CAST(lpep_pickup_datetime as timestamp), \n",
    "                CAST(lpep_dropoff_datetime as timestamp)) as DOUBLE) as duration_minutes,\n",
    "            CAST(trip_distance as DOUBLE) as trip_distance,\n",
    "            CAST(hour(CAST(lpep_pickup_datetime as timestamp)) as double) as hour_of_day\n",
    "        FROM {}\n",
    "        WHERE DAY(CAST(lpep_pickup_datetime as timestamp)) = {} -- Filter by day\n",
    "    )\n",
    ")\n",
    "ORDER BY error DESC\n",
    "LIMIT {};\n",
    "'''.format(endpoint_name, table_name, 1, 10)\n",
    "print('Querying...', query_sql)\n",
    "\n",
    "query_df = cursor.execute(query_sql, work_group=workgroup_name).as_pandas()\n",
    "print('Status: {}, Run time: {:.2f}s, Data scanned: {:.2f}MB, Records: {:,}'.format(cursor.state, \n",
    "    cursor.execution_time_in_millis/1000.0, cursor.data_scanned_in_bytes/1024.0/1024.0, query_df.shape[0]))\n",
    "\n",
    "query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}